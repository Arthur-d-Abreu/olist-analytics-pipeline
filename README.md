# ğŸ“Š Projeto de AnÃ¡lise de Dados â€” Olist E-commerce

Este projeto tem como objetivo realizar um pipeline completo de **engenharia e anÃ¡lise de dados**, partindo de dados brutos atÃ© a construÃ§Ã£o de um **modelo dimensional (Star Schema)** pronto para consumo em ferramentas de BI.

---

## ğŸš€ Objetivo do Projeto

Construir uma soluÃ§Ã£o completa de dados que permita:

- OrganizaÃ§Ã£o dos dados transacionais
- CriaÃ§Ã£o de um **Data Warehouse dimensional**
- GeraÃ§Ã£o de insights estratÃ©gicos
- VisualizaÃ§Ã£o clara atravÃ©s de dashboards

---

## ğŸ§± Arquitetura do Projeto
CSV â†’ Python (ETL) â†’ Banco de Dados Relacional â†’ Modelagem Dimensional (Star Schema) â†’ Power BI

---

## âš™ï¸ Tecnologias Utilizadas

- Python (Pandas, SQLAlchemy)
- SQL Server
- SQL (T-SQL)
- Power BI
- Git & GitHub

---

## ğŸ—‚ï¸ Estrutura do RepositÃ³rio

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Arquivos CSV originais
â”‚   â””â”€â”€ processed/      # Dados tratados prontos para carga
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ clean_data.py   # Limpeza e tratamento dos dados
â”‚   â””â”€â”€ upload_base.py  # Carga automatizada para o SQL Server
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ views.sql       # CriaÃ§Ã£o das views dimensionais e fato
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ dashboard.pbix  # Dashboard final(em construÃ§Ã£o)
â”‚
â””â”€â”€ README.md

---

## ğŸ› ï¸ Processo ETL
O pipeline ETL foi desenvolvido em Python utilizando Pandas e SQLAlchemy, implementando prÃ¡ticas de engenharia de dados, incluindo:

- PadronizaÃ§Ã£o de nomes de colunas
- Tratamento de valores nulos
- CorreÃ§Ã£o de tipos de dados
- CriaÃ§Ã£o automÃ¡tica de tabelas no banco
- Carga incremental em chunks para alta performance

O processo de carga Ã© totalmente automatizado, com suporte a mÃºltiplos bancos de dados (SQL Server, PostgreSQL, MySQL e SQLite), garantindo portabilidade e escalabilidade.

---
âš™ï¸ Como Executar o Projeto
### 1ï¸âƒ£ Criar ambiente virtual (opcional)
```bash
python -m venv venv
venv\Scripts\activate
```
### 2ï¸âƒ£ Instalar dependÃªncias
```bash
pip install -r requirements.txt
```
### 3ï¸âƒ£ Limpar os dados

python scripts/clean_data.py

### 4ï¸âƒ£ Subir os dados no banco
```bash
python scripts/upload_base.py --db-type sqlserver
```
### 5ï¸âƒ£ Criar as views dimensionais

Executar os scripts SQL localizados na pasta /sql no SQL Server.

## ğŸ§© Modelo Dimensional

O projeto implementa **modelagem dimensional em estrela (Star Schema)**, separando fatos e dimensÃµes para otimizar consultas analÃ­ticas e performance em BI.


### ğŸ”¹ Tabela Fato

- `vw_fato_vendas`

### ğŸ”¹ DimensÃµes

- `vw_dim_cliente`
- `vw_dim_produto`
- `vw_dim_vendedores`
- `vw_dim_tempo`

---

## ğŸ“ Regras de NegÃ³cio

### ClassificaÃ§Ã£o de tamanho de produto

Foi criado um critÃ©rio baseado no volume do produto:


| Volume (cmÂ³) | ClassificaÃ§Ã£o |
|---------------|---------------|
| â‰¤ 20.000      | Pequeno       |
| â‰¤ 100.000     | MÃ©dio         |
| > 100.000     | Grande        |

---

## ğŸ“Š PrÃ³xima Etapa

ConstruÃ§Ã£o de dashboards no **Power BI**, incluindo:

- Receita total
- Ticket mÃ©dio
- Vendas por estado
- Vendas por categoria
- DistribuiÃ§Ã£o logÃ­stica por tamanho

---

## ğŸ‘¨â€ğŸ’» Autor

Arthur Abreu  
Projeto desenvolvido para fins educacionais, validaÃ§Ã£o de conhecimentos em engenharia e anÃ¡lise de dados, e composiÃ§Ã£o de portfÃ³lio profissional.

--------------------------------------------------------------------------------------------------------------------------------------------------------

EN

# ğŸ“Š Data Analytics Project â€” Olist E-commerce

This project aims to build a complete **data engineering and analytics pipeline**, starting from raw data ingestion to the construction of a **dimensional data warehouse (Star Schema)**, ready for consumption by BI tools.

---

## ğŸš€ Project Objective

To build a complete data solution that enables:

- Organization of transactional data  
- Creation of a **dimensional Data Warehouse**  
- Generation of strategic insights  
- Clear and efficient visualization through dashboards  

---

## ğŸ§± Project Architecture

CSV â†’ Python (ETL) â†’ Relational Database â†’ Dimensional Modeling (Star Schema) â†’ Power BI

---

## âš™ï¸ Technologies Used

- Python (Pandas, SQLAlchemy)  
- SQL Server  
- SQL (T-SQL)  
- Power BI  
- Git & GitHub  

---

## ğŸ—‚ï¸ Repository Structure

â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original CSV files
â”‚ â””â”€â”€ processed/ # Cleaned data ready for loading
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ clean_data.py # Data cleaning and preprocessing
â”‚ â””â”€â”€ upload_base.py # Automated database loading
â”‚
â”œâ”€â”€ sql/
â”‚ â””â”€â”€ views.sql # Creation of fact and dimension views
â”‚
â”œâ”€â”€ powerbi/
â”‚ â””â”€â”€ dashboard.pbix # Final dashboard (in progress)
â”‚
â””â”€â”€ README.md

---

---

## ğŸ› ï¸ ETL Process

The ETL pipeline was developed in Python using Pandas and SQLAlchemy, following data engineering best practices, including:

- Column name standardization  
- Missing value handling  
- Data type correction  
- Automatic table creation  
- Chunk-based incremental loading for high performance  

The loading process is fully automated and supports multiple databases (SQL Server, PostgreSQL, MySQL, and SQLite), ensuring portability and scalability.

---

## âš™ï¸ How to Run the Project

### 1ï¸âƒ£ Create a virtual environment (optional)
```bash
python -m venv venv
venv\Scripts\activate
```
### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
### 3ï¸âƒ£ Clean the data

python scripts/clean_data.py

### 4ï¸âƒ£ Load data into the database
```bash
python scripts/upload_base.py --db-type sqlserver
```
### 5ï¸âƒ£ Create dimensional views

Executar os scripts SQL localizados na pasta /sql no SQL Server.

## ğŸ§© Dimensional Model

The project implements star schema dimensional modeling, separating fact and dimension tables to optimize analytical queries and BI performance.


### ğŸ”¹ Fact Table

- `vw_fato_vendas`

### ğŸ”¹ Dimensions

- `vw_dim_cliente`
- `vw_dim_produto`
- `vw_dim_vendedores`
- `vw_dim_tempo`

---

## ğŸ“ Business Rules

Product size classification

A business rule was created based on product volume:


| Volume (cmÂ³) | ClassificaÃ§Ã£o |
|---------------|---------------|
| â‰¤ 20.000      | Small         |
| â‰¤ 100.000     | Medium        |
| > 100.000     | Large         |

---

## ğŸ“Š Next Steps

Development of dashboards in Power BI, including:

- Total revenue
- Average ticket
- Sales by state
- Sales by category
- Logistics distribution by product size

---

## ğŸ‘¨â€ğŸ’» Author

Arthur Abreu
Project developed for educational purposes, validation of data engineering and analytics skills, and professional portfolio composition.


